IBM provides a collection of LLM's from the best of open source, 3rd party and IBM trained Foundation models. IBM Foundation Models span across encoder, encoder-decoder, decoder style models. IBM provided LLM's such as the Granite series are trained on data that includes a significant portion of financial and legal sources that enhance the relevancy of the generated text to financial contexts. watsonx.governance automates model metric collection and alerting.  The platform is agnostic to where models are trained and deployed and supports a wide variety of model frameworks from traditional Machine Learning to newer transformer architectures for Foundation Models and LLM's. The watsonx.ai is highly configurable and customizable.  Users have access to several configuration parameters in the UI and also via API.  The IDE includes low and no code tools as well as tools for programmers in python or R.  The watsonx platform integrates with open source and 3rd party software. IBM is developing model architectures for foundation models to achieve core design principles of efficiency and sustainability.  IBM Research has developed new algorithms such as “LiGO” that recycles small models and “grows” them into larger ones.  We are leveraging our expertise in quantization to shrink model size without sacrificing accuracy.  https://openreview.net/pdf?id=cDYRS5iZ16f Users can generate confidence scores to measure coherence  The platform is able to generate text that is contextually relevant and logical via prompt engineering and support for Retrieval Augmented Generation (RAG).  The grammar generated is highly dependent on the source within the LLM.  If additional capabilities are needed we provide API support for tools that have that specialized function.  Model parameters allow users to control diversity and creativity in text generation.    Model inference parameters enable users to control uniqueness, creativity, number of output tokens in generated text  IBM offers users the choice of LLM's from IBM and also multiple sources including Meta, Google, and Hugging Face.  IBM takes several steps in building IBM models to either filter copyright materials or obtain permissions to use.  IBM provides a model card that lists the data sources.  IBM also indemnifies customers against copyright infringement lawsuits when using IBM models.  The watsonx.governance service will provide out-of-the box Gen AI metrics including Rouge and BLEU.   IBM watsonx.ai is focused on enterprise oriented data.  Predominant use cases are centered around text and speech.  Today, watsonx does not support image generation.  The IBM speech services provide metrics on performance.  There is support for custom model metrics.  Models can be deployed for both batch and real-time inferencing  For each LLM, there are multiple parameters available to control model behavior such as Temperature, Repetition Penalty, Stopping Sequence, Min and Max Number of Tokens and Top K and Top P.  The Watsonx.ai platform provides no-code, low-code (AutoAI) and programming language support for a diverse set of user capabilities.  The watsonx.ai platform can integrate with vector databases and provides a repository for prompt tuning sessions.  The watsonx.ai platform prompt lab provides the capability for rapid Gen AI prototyping and the AutoAI application provides rapid prototyping for traditional ML problems.  Scaling of compute and storage is dynamic and responds to usage.  With watsonx.data where the data resides is not a factor in model training or operation.  The platform provides both CPU and GPU runtimes.  The watsonx platform is available as a SaaS solution today and will also include an on-prem option later this year.  The watsonx platform includes several pre-trained LLM's from IBM and other providers including Meta, Google and Hugging Face.  IBM has a partnership with Hugging Face to make available other open source LLM's.  For each LLM, there are multiple parameters available to control model behavior such as Temperature, Repetition Penalty, Stopping Sequence, Number of Tokens and Top K and Top P.  Models can be deployed for both batch and real-time inferencing using a Python Client or a user friendly UI.    Using watsonx.governance, users can upload feedback data or access feedback data in a data repository to asses accuracy.  The watsonx.ai includes several popular ML frameworks out of the box.  In addition, users can create custom runtimes to incorporate other frameworks.  IBM has deep expertise in quantization that is employed in model training. IBM is using quantization, multitask prompt tuning and new algorithms to compress models.  Model fine tuning is on the roadmap for 1H 2024.  When available, you will be able to deploy models over multi CPU/GPU environments.  The watsonx platform is deployed on a multi-node cluster and offers both CPU and GPU runtime environments.  The platform will support efficient compilers and runtime accelerators.  See Seismic Configurator.  Also contact Watsonx PM team  The Granite Foundation Models White Paper documents how Granite Models were evaluated and compared with other models.  In Part, General knowledge benchmarks, Helm Benchmarks, and Enterprise Benchmarks based on 11 Financial tasks were evaluated and reported on.   IBM provides approaches and tools to customize models from fine tuning (which is often most appropriate for smaller custom models) to prompt tuning to multi-task prompt tuning (a state-of-the-art approach to model customization). These capabilities will be in watsonx.ai (Tuning first, fine-tuning later) and are available directly through the underlying APIs. The key in model customization has been to provide the right user experience to guide a user to select which data is needed for customization, how much data, and how to configure the tuning job.   Inference engines  / frameworks Supported are TGIS and vLLM It is very easy to deploy an inferencing model from the model registry. A simple change to the CR suffices Inferencing API supports streaming TTFT depends on the input token and current concurrent requests in LLM queue.  On average, 150 ms. On average 20-30ms, which depends on the number of input and output token, and the concurrency of the requests. On average, for A/H100, 60 concurrent requests per model instance, and for LS40, 30 concurrent requests per model instance, while maximize the GPU usage.On average, 400 output token per second per model instance. you can either assign multiple GPUs from a single node to a single model inference pod, or split them between multiple instances. Concurrency will depend on the hardware configuration but we can scale concurrency to whatever needs there are.