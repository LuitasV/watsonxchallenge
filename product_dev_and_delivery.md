All involved parts of the solution are scanned regularly for known vulnerabilities, using multiple third-party scanning tools. Each of the Critical and High vulnerabilities are analyzed and assessed. Those that are assessed to have an impact are fixed. All released software parts are free of critical or high.   All involved parts of the solution are scanned for new vulnerabilities daily. Penetration tests on IBM Cloud on which watsonx.ai aaS is hosted & more are done regularly as mentioned here: https://cloud.ibm.com/docs/overview?topic=overview-security.  Critical and high vulnerabilities are addressed before releasing.  Vulnerabilities identified as a result of technical vulnerability tests and / or penetration tests must be remedied based on the following parameters:  * Emergency: As soon as possible  * Critical: 30 days  * Medium: 90 days  * Low: With a risk-based approach Emergency vulnerabilities are handled as quickly as possible via an exception process. Both critical and major vulnerabilities are resolved within a week. Medium vulnerabilities are remediated within 90 days, low vulnerabilities are addressed based on the risk they pose. Emergency vulnerabilities are handled as quickly as possible via an exception process. Both critical and major vulnerabilities are resolved within a week. Medium vulnerabilities are remediated within 90 days, low vulnerabilities are addressed based on the risk they pose.   New security patches available must be checked at least once a week Vulnerability scanning happens daily, with CVE libraries within the scanners updated continuously.     Watsonx.ai watson.ai is stateless when a prompt is inferred through the API, watsonx.ai does not store information. It is the responsibility of the users using watsonx.ai when using the User Interface to define a prompt or a data set not to input such data if they should not be stored in watsonx.ai  A collection of foundation models are deployed in IBM watsonx.ai. Links to details about each model, including pretraining data and fine-tuning, are available here: https://www.ibm.com/docs/en/watsonx-as-a-service?topic=models- information on governance practices related to IBM’s Granite Model can be Seen at Granite Foundation Models White Paper located here.  Additional information on all models available on watsonx can be found here:  https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-overview.html?context=wx IBM does not store the data being input into the Watson models, as it is pulled in via API and otherwise not stored  We do not store any data input to our models.  The inferencing API itself uses HTTPs and hence the data is encrypted on the wire  IBM does not use the data provided to further train its AI models or otherwise its own commercial purposes Client data remains theirs and will be treated no different than any other Content uploaded into any IBM Cloud Service.   the following modalities are convered by watsonx: Text, Semi structured, Unstructured data.   Images are a potential future enhancement and are also potentially modeled with ML, with results input to the FM.   IBM Research has developed a new multi-modal foundation model specifically for OCR that receives images with text and makes the text available in a Retrieval Augmented Generation workflow (RAG).  https://ocr.res.ibm.com/   watsonx is working with our customers to explore the commercial value of these type of innovative models.   Modalities can arrive independently.  Watsonx supports LangChain to make a series of calls to a language model.  https://dataplatform.cloud.ibm.com/exchange/public/entry/view/c3dbf23a-9a56-4c4b-8ce5-5707828fc981?context=wx   benchmark results on the skill of the model  can be Seen at Granite Foundation Models White Paper-Table IV.  In addition to benchmark, IBM provides sample prompts and a python library for common commercial use cases to accelerate the AI Builder’s prompt engineering experience.  https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-prompt-samples.html?context=wx&audience=wdp https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-python-lib.html?context=wx    IBM granite models go through a process called supervised fine-tuning (SFT) which is outline in the Granite Foundation Models White Paper-Table 1.  This process creates an model that is optimized to respond to instructions common in commercial use cases.   After SFT, IBM granite models go through a process called Constrastive Fine-tuning (CFT) which is outline in the Granite Foundation Models White Paper-Table 1.  This process aligns the model output to increase the probability of a positive response making the model more helpful, honest, and penalize the probability of negative data be reduce  an model that is optimized to respond to instructions common in commercial use cases.   IBM provides approaches and tools to customize models from fine tuning (which is often most appropriate for smaller custom models) to prompt tuning to multi-task prompt tuning (a state-of-the-art approach to model customization). These capabilities will be in watsonx.ai (Tuning available today, fine-tuning later) and also are available directly through the underlying APIs. The key in model customization has been to provide the right user experience to guide a user to select which data is needed for customization, how much data, and how to configure the tuning job.  Prompt tuning, fine tuning, and multi-task prompt tuning can be done in an AWS VPC environment upon release of Watsonx.ai on prem.   Training would generally be done by IBM, one reason is the training data is not published.  However we do have the ability to work on custom models, such as industry specific models, on a partnership basis.  IBM does publish descriptions of the training data.  The amount of time required for Prompt Tuning is a function of the model and the amount of data. Early benchmarking has shown a model in watsonx can be Prompt Tuned in as little as an hour on our SaaS infrastructure.  This infrastructure is run on A100s