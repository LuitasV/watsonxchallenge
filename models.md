We deliver models through docker registry The watsonx model registry has no parameter limitation and can support multiple foundation model architectures and models of any size & parameters.   Nginx is used for ingress and openshift routes are used as a passthrough, the settings are robust and throughput testing is regularly performed to check for bottlenecks For each foundation model on watsonx, users have access to its metadata, which may include: model parameter size, model type, languages supported, links to model provider's model card and Github repo, infrastructure requirements and usage guidance, intended vs unintended use cases, training data, performance benchmarks, environmental impact from training, and exposure to potential ethical/hateful content Yes  watsonx has several open source models available in our model registry on SaaS and thousands more available onpremise via our bring-your-own-model (BYOM) feature when deployed on-premise.  On SaaS, watsonx open source model providers include Meta, Google, Mistral, BigScience, and IBM's Granite series.   IBM's open-source models include granite-7b.   Meta models include codellama-34b-instruct (16K context), the Llama2 model series (llama-2-chat-70b, llama-2-chat-13b), and Meta's highest performing Llama3 series (llama-3-8B and llama-3-70b).  Google models include flan-ul2-20b, flan-t5-xxl-11b, and flan-t5-xl-3b.  Mistral models include mixtral-8x7b  and mixtral-8x7b quantized.  BigScience models include mt0-xxl-13b  The model registry for watsonx is continuously updated to reflect the latest performing open source models  Yes, watsonx hosts numerous open source and 3rd party models from reputable vendors such as Meta.  Also, the client may wish to import a model of their choice from hugging face or other source so long as the underlying model architecture is supported by our platform.  This feature is currently available in the software version and will be supported in the SaaS version in late Q2.  A list of non-IBM, 3rd party models currently available on the watsonx platform can be reviewed in Question 31.  Both our SaaS and onpremise deployments will be continually updated with the latest high performing models.  Yes, the model registry enables users to filter by task, model provider, and model type.  In addition, each model in the registry comes with metadata with information regarding acceptable use, model size, sustainability, and links their respective model cards and research papers   To ensure responsible and trusted model performance for the enterprise, IBM evaluates every proprietary and open source model against several benchmarks, such as general industry knowledge, IBM specific benchmarks, enterprise benchmarks, and model safety and red teaming benchmarks.  Further, many of these evaluations are completed across the various languages supported.  Note that some of these benchmarks apply to IBM proprietary Granite models only.  General industry knowledge benchmarks include question answering for several domains (boolq, openbookqa, piqa, sciq); sentence completion (lambada), commonsense reasoning (arc easy, arc challenge, copa, hellaswag, winogrande); reading comprehension (race); and multidisciplinary multiple-choice collection (mmlu)  IBM benchmarks leverage proprietary datasets to evaluate performance against common enterprise use cases, specifically classification, entity extraction, and document & dialog summarization.   Enterprise benchmarks evaluate the model's performance against 10 finance/insurance-specific tasks relevant to enterprise use cases  around sentiment classification, classification, entity recognition, Q&A, and summarization.   Safety and red-teaming benchmarks evaluate a model's bias via the BOLD benchmark, its harmlessness evaluated using IBM's AttaQ dataset, and social bias using the SocialStigmaQA benchmark.  Additional details and the output of our Granite model benchmarks can be reviewed here: https://www.ibm.com/downloads/cas/X9W4O6BM.  In the second half of this year, to enable users to make informed model selection decisions, IBM will be making select benchmark evaluations visible within the platform and enable users to evaluate fine-tuned models within the platform  Yes, a cluster admin can unregister the model by removing it from custom resource definition yaml file. Models are only accessed via the API, the platform can provide granular access control for users in projects/spaces which can control who has access to the models We use docker registry to deliver models  Watsonx fully supports both LangChain and llama-index and we are developing additional technologies in this space.  The platform hosts notebooks that have LangChain in them, and can run those as batch jobs, or create AI functions that do the same. We have a built-in operational lifecycle management option in the watsonx inference component where you can pre-set availability, deprecation and removal dates for LLM inference  models. In addition, watsonx.governance model inventory can be used to capture model facts throughout the entire model lifecycle all the way from model request to model development, validation,  approval, deployment, monitoring and retirement.  Watsonx provide Git integration with projects so that all assets listed in the project reflect the current state of your Git clone.  Also, watsonx has deployment space that can manage the foundation model registry either programmatically or through UI. References: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=repositories-default-git-integration;  https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=assets-deploying-ai   the product has an automated process for deployment  watsonx provides Model Explainability watsonx provides Model Documentation watsonx provides Testing and Validation of models watsonx provides Model robustness and generalization In addition to context attribution for LLM RAG use cases, watsonx supports multiple levels of explainability for Machine Learning models - specifically,  local explanations to understand the impact of factors for specific transactions or global explanations to understand the general factors that impact model outcomes. It also includes Contrastive explanations that enables users to run 'What-If' scenario analysis to further understand model outcomes, enabling end users to test their hypothesis.  Watsonx has AI Factsheet which is documentation automatically created for models as it progresses from build and train to validation and deploy. For example, for ML model build with watsonx AutoAI, automatically captures model description, algorithm and model type, link to training dataset, input/output schema of training data, evaluation and monitoring metrics. AI use cases in watsonx include test and validation stages prior to production to help ensure quality and proper approvals before model is deployed in production.  Each version at each stage has associated AI Factsheet that has key metadata such as quality, accuracy, and performance testing that model validators can review easily before moving to production.   For adversarial robustness, IBM's opensource Adversarial Robustness toolbox can be configured as a 'custom' metric in watsonx.  For generalization, quality and accuracy metric computed on different datasets can inform generalization.   IBM Granite model is a general-purpose foundation model suited for a wide range of business targeted generative tasks with unseen data such as summarization, content generation, retrieval-augmented generation, classification, and extracting insights.   Model Registry (enterprise-wide) and artifact management Automated publishing from the development environment to a central model registry Model Deployment Model Tracking Model Performance Monitoring (drift) and Logging Monitoring of external models (built outside your ML platform) Support for model export to other MLOps platforms Alerting and Notifications Model Retraining Model Serving as endpoints Autoscaling of infrastructure to meet varying workloads Deployment options for batch, real-time, and streaming  Watsonx has a model registry referred to as  Spaces  where users across the same account can deploy models and manage model assets as well as monitor/schedule jobs, monitor usage, track history of model deployments. Other artifacts can be viewed and managed in the Projects user interface where models are either imported or developed.  Watsonx has a user interface that promotes (i.e. publish) a model with one click.  Also, within Pipelines, a model can be promoted to a Space and deployed automatically.. Watsonx can deploy machine learning models, scripts, functions, and prompt templates. After creating deployments, you can test and manage them, and prepare assets to deploy into pre-production and production environments.   With watsonx, models including different versions of the same model can be tracked in Inventory and as part of Use Cases across its lifecycle starting from build to deploy. Each tracked model has an associated AI Factsheet which reflects automatically captured key 'facts' or metadata such as model type, link to the training dataset, values for metrics the user has enabled monitoring for, and any other 'custom' fact the user would like to capture. Users can also add attachments to these AI Factsheet and can export them as well.   Within the deployment spaces, users can track job status  across environments as well as monitor resource usage.   watsonx monitors drift in model prediction, quality (accuracy) drift, and feature drift. Thresholds can be set for drift and users alerted if there is a breach.  It also captures several out-of-the-box model performance metrics such as scoring requests, records, throughput and latency, payload size, and number of users sending scoring requests. Users can also add 'custom' metrics to capture additional performance metrics based on their needs. Both drift and performance metrics can be visualized in the Insights dashboard as well.  In addition to monitoring LLMs outside watsonx, watsonx can perform payload logging, feedback logging, measure accuracy, quality, performance, and fairness for ML models build and deployed outside watsonx. It supports several out-of-the-box ML frameworks (AzureML, AWS Sagemaker) or any the user chooses through 'custom' set-up.  These configurations can be done either through UI or API.  Metrics are reflected in the UI as well so users can still visualize the results, and leverage explainability capabilities. Watsonx allows ML models to be exported.  User can export a model if it was originally generated in the platform with Auto AI or through code with opensource. Also, if a client imports a model to the platform, they will be able later export it when desired.  Watsonx provides alerts and notifications which can be configured for a number of different functionality in the platform.  For model quality, fairness, performance metrics,  alerts are generated for threshold violations and users are notified by email or SLACK. These alerts can also be sent to other services like GiHub, ServiceNow, etc. through WebHooks.  Similarly, alerts and notifications can also be configured for workflows and issue management capabilities. Watsonx provides model retraining through pipelines and modeler flows for machine learning models. Both can be set up as jobs in the system. There is also the capability to monitor model performance metrics which can generate alerts when thresholds are met. Users will be notified and can manually execute the corresponding pipeline or modeler flow jobs to retrain a model.  Watsonx provides model deployment capabilities which generate an endpoint for both online and batch serving.   Watsonx utilizes a multi-tenant queue based system with an adjustable number of worker nodes to optimize workloads. Both single and multi GPU jobs can be run concurrently based on the workload to optimize throughput and minimize latency.  Watsonx SaaS platform leverages a variety of GPU types to match workload requirements with cost-effective hardware.  Also, the platformâ€™s flexible deployment capabilities brings workloads closer to data which improves latency.  For optimizing model inference, IBM has designed its foundation model library with a focus on smaller, domain or task-specific LLMs that require much lower compute and provide lower response latency.  Also, models can be deployed with a multi-pod deployment allowing more GPU resources to be used for a single model in case of high load. Requests from single or multiple users are dealt with concurrently in an optimized batch manner through the use of TGI or vLLM based model runtimes.    Watsonx provides deployment for both batch and realtime (i.e. online). Streaming is on the roadmap.  Watsonx has fully integrated foundation models with both code and no-code methods to access them. Watsonx supports IBM developed foundation models, the Granite and Slate series of models.  It also supports open source models, primarily from Hugging Face. There are also several third party models from leading model vendors such as Meta, Mistral and others.  Clients can also import their own custom foundation models that are based on supported model architectures.  Watsonx supports language models, both text and code generation.  There are libraries available to support text to speech.  Other modalities such as text to image, image to text are on the roadmap for Q2 and Q3 2024.   Watsonx platform hosts various code generation models which can be leveraged to create executable code to retrain a model, e.g. when monitoring metrics reach a given threshold.  Watsonx platform utilizes LLMs for it's AI Guardrails (e.g. filtering out hate speech, abuse and profanity)